# Adversarial Attacks on Neural Networks ğŸ›¡ï¸

This repository contains code and experiments evaluating adversarial attacks and defenses on CNN models trained on CIFARâ€‘10. The goal was to build robust image classifiers through **Min-Max adversarial training**.

---

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ `CIFAR (3).ipynb`          # Baseline CNN for CIFAR-10
â”‚   â””â”€â”€ `Adversarial_attck (1).ipynb` # FGSM evaluation & Min-Max robust training
â”œâ”€â”€ data/                         # Scripts/notebooks to download or preprocess CIFAR-10
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ `model.py`                # CNN architecture definition
â”‚   â”œâ”€â”€ `train_baseline.py`       # Code to train baseline CNN
â”‚   â”œâ”€â”€ `fgsm_attack.py`          # FGSM attack implementation
â”‚   â””â”€â”€ `train_robust.py`         # Min-Max adversarial training routine
â”œâ”€â”€ images/                       # Figures for accuracy curves, confusion matrices, etc.
â”‚   â”œâ”€â”€ `confusion_matrix_baseline.png`
â”‚   â”œâ”€â”€ `accuracy_vs_epsilon_baseline.png`
â”‚   â””â”€â”€ `â€¦`
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## ğŸ¯ Objectives & Results

- **Baseline CNN** achieved **78.6%** accuracy on clean CIFAR-10.
- Under FGSM perturbations:
  - With Îµ = 0.001 â accuracy dropped to ~43.3%
  - With Îµ = 0.01 â accuracy dropped to ~2.3%
- **Robust model** trained using Min-Max adversarial training:
  - Clean accuracy: **69.3%**
  - Îµ = 0.001 â **55.2%**, Îµ = 0.01 â **4.7%**

ğŸ“ˆ These results highlight the effectiveness of adversarial trainingâ€”while clean accuracy slightly decreases, robustness against attacks improves significantly.

---

## ğŸš€ Installation

1. **Clone the repo:**
   ```bash
   git clone https://github.com/anirudhksharma/Adverseral_Attacks_on_Neural_Net.git
   cd Adverseral_Attacks_on_Neural_Net
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Download CIFAR-10 data:**  
   Use the notebook in `notebooks/` or modify scripts in `data/`.

---

## ğŸ”§ Usage

- **Train baseline model:**
  ```bash
  python src/train_baseline.py
  ```

- **Evaluate FGSM attack on baseline:**
  ```bash
  python src/fgsm_attack.py --model baseline
  ```

- **Train robust model with Min-Max FGSM:**
  ```bash
  python src/train_robust.py --epsilon 0.001
  ```

- **Evaluate robust model under attack:**
  ```bash
  python src/fgsm_attack.py --model robust --epsilon 0.001
  ```

---

## ğŸ“š Background

The **FGSM** attack (Goodfellow et al., 2015) perturbs inputs via:
```
x_adv = x + Îµ Â· sign(âˆ‡_x L(f(x), y))
```
**Min-Max adversarial training** seeks to solve:
\[
\min_Î¸ \, E_{x,y}\left[\max_{\|Î´\|â‰¤Îµ} L(f_Î¸(x+Î´),y)
ight]
\]
This approach improves resilience against perturbations.

### Other common defense methods:
- Defensive Distillation
- Gradient Masking
- Randomized Smoothing

ğŸ§  Read more in `Comparison with Other Defense Methods` section in the main report.

---

## ğŸ“– References

- Goodfellow, I. J., Shlens, J., & Szegedy, C. (2015). *Explaining and Harnessing Adversarial Examples*. arXiv:1412.6572
- Madry, A., et al. (2018). *Towards Deep Learning Models Resistant to Adversarial Attacks*. arXiv:1706.06083
- Papernot, N., et al. (2017). *Ensemble Adversarial Training: Attacks and Defenses*. arXiv:1705.07204

---

## ğŸ“ License

This work is licensed under the MIT License. See [LICENSE](LICENSE) for details.

